@inproceedings{SIGCOMM-20,
  author = {Gallo, Massimo and Finamore, Alessandro and Simon, Gwendal and Rossi, Dario},
  title = {Real-time Deep Learning based Traffic Analytics},
  year = {2020},
  booktitle = {Proceedings of the SIGCOMM Poster and Demo Sessions},  
  venue = {Online},
  month = {August},
  abstract = {The increased interest towards Deep Learning technologies has led to the development of a new generation of specialized hardware accelerators, including edge TPUs, which are expected to yield a higher ratio of operations per watt footprint than GPUs. In this demo we investigate the performance of edge TPU for traffic analytics. We sketch the design of a real-time DL traffic classification system running a state-of-the art CNN, and compare inference speed on different hardware (CPU, GPU, TPU).We run stress tests based on synthetic traffic and under different conditions, and collect the results into a dashboard enabling both data exploration and to launch synthetic live tests on top of Ascend 310 TPUs.},
  howpublished = {/docs/2020sigcomm.pdf},
  topic = {inference},
  series = {SIGCOMM-WS 20}
}

@inproceedings{CoNEXT-21,
  author = {Azorin, Raphaël and Gallo, Massimo and Finamore, Alessandro and Maurizio, Filippone and Pietro, Michiardi and Rossi, Dario},
  title = {Towards a Generic Deep Learning Pipeline for Traffic Measurements},
  year = {2021},
  booktitle = {Proceedings of the 2nd International CoNEXT Student Workshop},
  month = {December},
  abstract = {As networks grow bigger, traffic measurements become more and more challenging. Common practices require specialized solutions tied to specific measurements. We aim at automating the design of generic top-down measurements tools thanks to Deep Learning. To this end, we focus our study on ($i$) researching an appropriate input traffic representation and ($ii$) comparing Deep Learning pipelines for several measurements. In this paper, we propose an empirical campaign to study a variety of modeling approaches for multiple traffic metrics predictions, with a strong focus on the trade-off between performance and cost that these approaches offer.},
  howpublished = {/docs/2021Conext.pdf},
  topic = {representation_learning},
  series = {CoNEXT-SW 21}
}


@inproceedings{SEC-21,
  author = {Gallo, Massimo and Finamore, Alessandro and Simon, Gwendal and Rossi, Dario},
  title = {FENXI: Deep-learning Traffic Analytics at the edge},
  year = {2021},
  booktitle = {Proceedings of ACM/IEEE Symposium on Edge Computing},
  venue = {San Jose},
  month = {December},
  abstract = {Live traffic analysis at the first aggregation point in the ISP network enables the implementation of complex traffic engineering policies but is limited by the scarce processing capabilities, especially for Deep Learning (DL) based analytics. The introduction of specialized hardware accelerators i.e., Tensor Processing Unit (TPU), offers the opportunity to enhance processing capabilities of network devices at the edge. Yet, to date, no packet processing pipeline is capable of offering DL-based analysis capabilities in the data-plane, without interfering with network operations. In this paper, we present FENXI, a system to run complex analytics by leveraging TPU. The design of FENXI decouples forwarding operations and traffic analytics which operates at different granularities i.e., packet and flow levels. We conceive two independent modules that asynchronously communicate to exchange network data and analytics results, and design data structures to extract flow level statistics without impacting per-packet processing. We prototyped and evaluated FENXI on general-purpose servers considering both both adversarial and realistic network conditions. Our analysis shows that FENXI can sustains 100 Gbps line rate traffic processing requiring only limited resources, while also dynamically adapting to variable network conditions},
  howpublished = {/docs/2021sec.pdf},
  topic = {inference},
  series = {SEC 21}
}

@inproceedings{CoNEXT-22,
  author = {Monterubbiano, Andrea and Azorin, Raphael and Castellano, Gabriele and Gallo, Massimo and Pontarelli, Salvatore},
  title = {Learned Data Structures for Per-Flow Measurements},
  month={December},
  booktitle = {Proceedings of the 3rd International CoNEXT Student Workshop},
  year = {2022},
  abstract = {This work presents a generic framework that exploits learning to improve the quality of network measurements. The main idea of this work is to reuse measures collected by the network monitoring tasks to train an ML model that learns some per-flow characteristics and improves the measurement quality re-configuring the memory according to the learned information. We applied this idea to two different monitoring tasks, we identify the main issues related to this approach and we present some preliminary results},   
  howpublished = {/docs/2022CoNEXT_SW.pdf},
  topic = {coda},
  series = {CoNEXT-SW 22}
}

@inproceedings{INFOCOM-22,
  title = {Accelerating Deep Learning Classification with Error-controlled Approximate-key Caching},
  author = {Finamore, Alessandro and Roberts, James and Gallo, Massimo and Rossi, Dario,}, 
  abstract = {While Deep Learning (DL) technologies are a promising tool to solve networking problems that map to classification tasks, their computational complexity is still too high with respect to real-time traffic measurements requirements. To reduce the DL inference cost, we propose a novel caching paradigm, that we named approximate-key caching, which returns approximate results for lookups of selected input based on cached DL inference results. While approximate cache hits alleviate DL inference workload and increase the system throughput, they however introduce an approximation error. As such, we couple approximate-key caching with an error-correction principled algorithm, that we named auto-refresh.  We analytically model our caching system performance for classic LRU and ideal caches, we perform a trace-driven evaluation of the expected performance, and we compare the benefits of our proposed approach with the state-of-the-art similarity caching -- testifying the practical interest of our proposal.},
  year = {2022},
  booktitle = {Proceedings of IEEE INFOCOM - IEEE Conference on Computer Communications},
  month = {may},
  topic = {inference},
  howpublished = {/docs/2022Infocom.pdf},
  series = {INFOCOM 22}
}

@inproceedings{HotNets-22,
  author = {Zied, Ben Houidi and Raphael, Azorin and Massimo, Gallo and Alessandro, Finamore and Dario, Rossi},
  title = {Towards a systematic multi-modal representation learning for network data},
  month={November},
  booktitle = {Proceedings of the 21st ACM Workshop on Hot Topics in Networks},
  year = {2022},
  abstract = {Learning the right representations from complex input data is the key ability of successful machine learning (ML) models. The latter are often tailored to a specific data modality. For example, recurrent neural networks (RNNs) were designed having the processing of sequential data in mind, while convolutional neural networks (CNNs) were designed to exploit spatial correlation naturally present in images. Unlike computer vision (CV) and natural language processing (NLP), each of which targets a single well-defined modality, network ML problems often have a mixture of data modalities as input. Yet, instead of exploiting such abundance, practitioners tend to rely on sub-features thereof, reducing the problem on single modality for the sake of simplicity. In this paper, we advocate for exploiting all the modalities naturally present in network data. As a first step, we observe that network data systematically exhibits a mixture of quantities (e.g., measurements), and entities (e.g., IP addresses, names, etc.). Whereas the former are generally well exploited, the latter are often underused or poorly represented (e.g., with one-hot encoding). We propose to systematically leverage state of the art embedding techniques to learn entity representations, whenever significant sequences of such entities are historically observed. Through two diverse usecases, we show that such entity encoding can benefit and naturally augment classic quantity-based features.},   
  howpublished = {/docs/2022HotNets.pdf},
  topic = {representation_learning},
  series = {HotNets 22}
}

@article{CoNEXT23,
  author = {Monterubbiano, Andrea and Azorin, Raphael and Castellano, Gabriele and Gallo, Massimo, and Rossi, Dario and Pontarelli, Salvatore},
  title = {SPADA: A Sparse Approximate Data Structure representation for lightweight per-flow monitoring},
  year = {2023},
  journal = {Proceedings of the ACM on Networking},
  abstract = {Accurate per-flow monitoring is critical for precise network diagnosis, performance analysis, and network operation and management in general. However, the limited amount of memory available on modern programmable devices and the large number of active flows force practitioners to monitor only the most relevant flows with approximate data structures, limiting their view of network traffic. We argue that, due to the skewed nature of network traffic, such data structures are, in practice, heavily underutilized, i.e., sparse, thus wasting a significant amount of memory. This paper proposes a Sparse Approximate Data Structure (SPADA) representation that leverages sparsity to reduce the memory footprint of per-flow monitoring systems in the data plane while preserving their original accuracy. SPADA representation can be integrated into a generic per-flow monitoring system and is suitable for several measurement use cases. We prototype SPADA in P4 for a commercial FPGA target and test our approach with a custom simulator that we make publicly available, on four real network traces over three different monitoring tasks. Our results show that SPADA achieves 2× to 11× memory footprint reduction with respect to the state-of-the-art while maintaining the same accuracy, or even improving it.},
  howpublished = {/docs/2023SPADA.pdf},
  topic = {coda},
  series = {CoNEXT 23}
}

@article{CoNEXT24,
  author = {Azorin, Raphael and Monterubbiano, Andrea and  Castellano, Gabriele and Gallo, Massimo, and Rossi, Dario and Pontarelli, Salvatore},
  title = {Taming the Elephants: Affordable Flow Length Prediction in the Data Plane},
  year = {2024},
  journal = {Proceedings of the ACM on Networking},
  abstract = {Machine Learning (ML) shows promising potential for enhancing networking tasks. In particular, early flow size prediction would be beneficial for a wide range of use cases. However, implementing an ML-enabled system is a challenging task due to network devices limited resources. Previous works have demonstrated the feasibility of running simple ML models in the data plane, yet their integration in a practical end-to-end system is not trivial. Additional challenges in resources management and model maintenance need to be addressed to ensure the network task(s) performance improvement justifies the system overhead. In this work, we propose DUMBO, a versatile end-to-end system to generate and exploit flow size hints at line rate.Our system seamlessly integrates and maintains a simple ML model that offers early coarse-grain flow size prediction in the data plane. We evaluate the proposed system on flow scheduling, per-flow packet inter-arrival time distribution, and flow size estimation using real traffic traces, and perform experiments using an FPGA prototype running on an AMD(R)-Xilinx(R) Alveo U280 SmartNIC. Our results show that DUMBO outperforms traditional state-of-the-art approaches by equipping network devices data planes with a lightweight ML model.},
  topic = {coda},
  series = {CoNEXT 24}
}


@inproceedings{AAAI-23,
  author = {Azorin, Raphael and Gallo, Massimo and Finamore, Alessandro and Rossi, Dario and Michiardi, Pietro},
  title = {"It's a Match!" - A Benchmark of Task Affinity Scores for Joint Learning},
  month={January},
  year = {2023},
  abstract = {While the promises of Multi-Task Learning (MTL) are attractive, characterizing the conditions of its success is still an open problem in Deep Learning. Some tasks may benefit from being learned together while others may be detrimental to one another. From a task perspective, grouping cooperative tasks while separating competing tasks is paramount to reap the benefits of MTL, i.e., reducing training and inference costs. Therefore, estimating task affinity for joint learning is a key endeavor. Recent work suggests that the training conditions themselves have a significant impact on the outcomes of MTL. Yet, the literature is lacking of a benchmark to assess the effectiveness of tasks affinity estimation techniques and their relation with actual MTL performance. In this paper, we take a first step in recovering this gap by (i) defining a set of affinity scores by both revisiting contributions from previous literature as well presenting new ones and (ii) benchmarking them on the Taskonomy dataset. Our empirical campaign reveals how, even in a small-scale scenario, task affinity scoring does not correlate well with actual MTL performance. Yet, some metrics can be more indicative than others.},   
  booktitle = {AAAI - 2nd International Workshop on Practical Deep Learning in the Wild},
  howpublished = {/docs/2023AAAIWorkshop.pdf},
  topic = {representation_learning},
  series = {AAAI-WS 23}
}

@inproceedings{wangchao2025ICLR,
  title={Fine-grained Attention in Hierarchical Transformers for Tabular Time-series}, 
  author={Wang, Chao and Franzese, Giulio Finamore, Alessandro and Gallo, Massimo and Michiardi, Pietro},
  year={2025},
  abstract="Diffusion models for Text-to-Image (T2I) conditional generation have recently achieved tremendous success. Yet, aligning these models with user's intentions still involves a laborious trial-and-error process, and this challenging alignment problem has attracted considerable attention from the research community. In this work, instead of relying on fine-grained linguistic analyses of prompts, human annotation, or auxiliary vision-language models, we use Mutual Information (MI) to guide model alignment. In brief, our method uses self-supervised fine-tuning and relies on a point-wise (MI) estimation between prompts and images to create a synthetic fine-tuning set for improving model alignment. Our analysis indicates that our method is superior to the state-of-the-art, yet it only requires the pre-trained denoising network of the T2I model itself to estimate MI, and a simple fine-tuning strategy that improves alignment while maintaining image quality.",
  booktitle={In International Conference on Learning Representations 2025},
  series = {ICLR 25}
}

@inproceedings{azorin2024MILETS,
  title={Fine-grained Attention in Hierarchical Transformers for Tabular Time-series}, 
  author={Raphael, Azorin and Zied, Ben Houidi and Massimo, Gallo and Alessandro, Finamore and Pietro, Michiardi},
  year={2024},
  abstract="Tabular data is ubiquitous in many real-life systems. In particular, time-dependent tabular data, where rows are chronologically related, is typically used for recording historical events, e.g., financial transactions, healthcare records, or stock history. Recently, hierarchical variants of the attention mechanism of transformer architectures have been used to model tabular time-series data. At first, rows (or columns) are encoded separately by computing attention between their fields. Subsequently, encoded rows (or columns) are attended to one another to model the entire tabular time-series. While efficient, this approach constrains the attention granularity and limits its ability to learn patterns at the field-level across separate rows, or columns. We take a first step to address this gap by proposing Fieldy, a fine-grained hierarchical model that contextualizes fields at both the row and column levels. We compare our proposal against state of the art models on regression and classification tasks using public tabular time-series datasets. Our results show that combining row-wise and column-wise attention improves performance without increasing model size. Code and data are available at this https URL.",
  booktitle={In KDD Workshop Mining and Learning from Time-Series},
  howpublished = {/docs/2024MileTS.pdf},
  series = {MILETS 24}
}

@inproceedings{azorin2024PracticalDL,
  title={Fine-grained Attention in Hierarchical Transformers for Tabular Time-series}, 
  author={Raphael, Azorin and Massimo, Gallo and Alessandro, Finamore and Pietro, Michiardi and Dario, Rossi},
  year={2023},
  abstract="Tabular data is ubiquitous in many real-life systems. In particular, time-dependent tabular data, where rows are chronologically related, is typically used for recording historical events, e.g., financial transactions, healthcare records, or stock history. Recently, hierarchical variants of the attention mechanism of transformer architectures have been used to model tabular time-series data. At first, rows (or columns) are encoded separately by computing attention between their fields. Subsequently, encoded rows (or columns) are attended to one another to model the entire tabular time-series. While efficient, this approach constrains the attention granularity and limits its ability to learn patterns at the field-level across separate rows, or columns. We take a first step to address this gap by proposing Fieldy, a fine-grained hierarchical model that contextualizes fields at both the row and column levels. We compare our proposal against state of the art models on regression and classification tasks using public tabular time-series datasets. Our results show that combining row-wise and column-wise attention improves performance without increasing model size. Code and data are available at this https URL.",
  booktitle={In AAAI Workshop on Practical Deep Learning in the Wild},
  howpublished = {/docs/2023PracticalDL.pdf},
  series = {PracticalDL 24}
}