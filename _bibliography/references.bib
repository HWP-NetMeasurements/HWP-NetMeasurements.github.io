@inproceedings{SIGCOMM-20,
  author = {Gallo, Massimo and Finamore, Alessandro and Simon, Gwendal and Rossi, Dario},
  title = {Real-time Deep Learning based Traffic Analytics},
  year = {2020},
  booktitle = {SIGCOMM 2020},
  venue = {Online},
  month = {August},
  abstract = {The increased interest towards Deep Learning technologies has led to the development of a new generation of specialized hardware accelerators, including edge TPUs, which are expected to yield a higher ratio of operations per watt footprint than GPUs. In this demo we investigate the performance of edge TPU for traffic analytics. We sketch the design of a real-time DL traffic classification system running a state-of-the art CNN, and compare inference speed on different hardware (CPU, GPU, TPU).We run stress tests based on synthetic traffic and under different conditions, and collect the results into a dashboard enabling both data exploration and to launch synthetic live tests on top of Ascend 310 TPUs.},
  howpublished = {/docs/2020sigcomm.pdf},
  topic = {inference},
  series = {SIGCOMM-WS 20}
}

@inproceedings{CoNEXT-21,
  author = {Azorin, Raphaël and Gallo, Massimo and Finamore, Alessandro and Maurizio, Filippone and Pietro, Michiardi and Rossi, Dario},
  title = {Towards a Generic Deep Learning Pipeline for Traffic Measurements},
  year = {2021},
  booktitle = {ACM Conext, Student workshop},
  month = {December},
  abstract = {As networks grow bigger, traffic measurements become more and more challenging. Common practices require specialized solutions tied to specific measurements. We aim at automating the design of generic top-down measurements tools thanks to Deep Learning. To this end, we focus our study on ($i$) researching an appropriate input traffic representation and ($ii$) comparing Deep Learning pipelines for several measurements. In this paper, we propose an empirical campaign to study a variety of modeling approaches for multiple traffic metrics predictions, with a strong focus on the trade-off between performance and cost that these approaches offer.},
  howpublished = {/docs/2021Conext.pdf},
  topic = {representation_learning},
  series = {CoNEXT-SW 21}
}


@inproceedings{SEC-21,
  author = {Gallo, Massimo and Finamore, Alessandro and Simon, Gwendal and Rossi, Dario},
  title = {FENXI: Deep-learning Traffic Analytics at the edge},
  year = {2021},
  booktitle = {ACM/IEEE Symposium on Edge Computing},
  venue = {San Jose},
  month = {December},
  abstract = {Live traffic analysis at the first aggregation point in the ISP network enables the implementation of complex traffic engineering policies but is limited by the scarce processing capabilities, especially for Deep Learning (DL) based analytics. The introduction of specialized hardware accelerators i.e., Tensor Processing Unit (TPU), offers the opportunity to enhance processing capabilities of network devices at the edge. Yet, to date, no packet processing pipeline is capable of offering DL-based analysis capabilities in the data-plane, without interfering with network operations. In this paper, we present FENXI, a system to run complex analytics by leveraging TPU. The design of FENXI decouples forwarding operations and traffic analytics which operates at different granularities i.e., packet and flow levels. We conceive two independent modules that asynchronously communicate to exchange network data and analytics results, and design data structures to extract flow level statistics without impacting per-packet processing. We prototyped and evaluated FENXI on general-purpose servers considering both both adversarial and realistic network conditions. Our analysis shows that FENXI can sustains 100 Gbps line rate traffic processing requiring only limited resources, while also dynamically adapting to variable network conditions},
  howpublished = {/docs/2021sec.pdf},
  topic = {inference},
  series = {SEC 21}
}

@inproceedings{CoNEXT-22,
  author = {Monterubbiano, Andrea and Azorin, Raphael and Castellano, Gabriele and Gallo, Massimo and Pontarelli, Salvatore},
  title = {Learned Data Structures for Per-Flow Measurements},
  month={December},
  year = {2022},
  abstract = {This work presents a generic framework that exploits learning to improve the quality of network measurements. The main idea of this work is to reuse measures collected by the network monitoring tasks to train an ML model that learns some per-flow characteristics and improves the measurement quality re-configuring the memory according to the learned information. We applied this idea to two different monitoring tasks, we identify the main issues related to this approach and we present some preliminary results},   
  howpublished = {/docs/2022CoNEXT_SW.pdf},
  topic = {coda},
  series = {CoNEXT-SW 22}
}

@inproceedings{INFOCOM-22,
  title = {Accelerating Deep Learning Classification with Error-controlled Approximate-key Caching},
  author = {Finamore, Alessandro and Roberts, James and Gallo, Massimo and Rossi, Dario,}, 
  abstract = {While Deep Learning (DL) technologies are a promising tool to solve networking problems that map to classification tasks, their computational complexity is still too high with respect to real-time traffic measurements requirements. To reduce the DL inference cost, we propose a novel caching paradigm, that we named approximate-key caching, which returns approximate results for lookups of selected input based on cached DL inference results. While approximate cache hits alleviate DL inference workload and increase the system throughput, they however introduce an approximation error. As such, we couple approximate-key caching with an error-correction principled algorithm, that we named auto-refresh.  We analytically model our caching system performance for classic LRU and ideal caches, we perform a trace-driven evaluation of the expected performance, and we compare the benefits of our proposed approach with the state-of-the-art similarity caching -- testifying the practical interest of our proposal.},
  year = {2022},
  month = {may},
  topic = {inference},
  howpublished = {/docs/2022Infocom.pdf},
  series = {INFOCOM 22}
}

@inproceedings{HotNets-22,
  author = {Zied, Ben Houidi and Raphael, Azorin and Massimo, Gallo and Alessandro, Finamore and Dario, Rossi},
  title = {Towards a systematic multi-modal representation learning for network data},
  month={November},
  year = {2022},
  abstract = {Learning the right representations from complex input data is the key ability of successful machine learning (ML) models. The latter are often tailored to a specific data modality. For example, recurrent neural networks (RNNs) were designed having the processing of sequential data in mind, while convolutional neural networks (CNNs) were designed to exploit spatial correlation naturally present in images. Unlike computer vision (CV) and natural language processing (NLP), each of which targets a single well-defined modality, network ML problems often have a mixture of data modalities as input. Yet, instead of exploiting such abundance, practitioners tend to rely on sub-features thereof, reducing the problem on single modality for the sake of simplicity. In this paper, we advocate for exploiting all the modalities naturally present in network data. As a first step, we observe that network data systematically exhibits a mixture of quantities (e.g., measurements), and entities (e.g., IP addresses, names, etc.). Whereas the former are generally well exploited, the latter are often underused or poorly represented (e.g., with one-hot encoding). We propose to systematically leverage state of the art embedding techniques to learn entity representations, whenever significant sequences of such entities are historically observed. Through two diverse usecases, we show that such entity encoding can benefit and naturally augment classic quantity-based features.},   
  howpublished = {/docs/2022HotNets.pdf},
  topic = {representation_learning},
  series = {HotNets 22}
}

@inproceedings{CoNEXT23,
  author = {Monterubbiano, Andrea and Azorin, Raphael and Castellano, Gabriele and Gallo, Massimo, and Rossi, Dario and Pontarelli, Salvatore},
  title = {SPADA: A Sparse Approximate Data Structure representation for lightweight per-flow monitoring},
  year = {2023},
  abstract = {Accurate per-flow monitoring is critical for precise network diagnosis, performance analysis, and network operation and management in general. However, the limited amount of memory available on modern programmable devices and the large number of active flows force practitioners to monitor only the most relevant flows with approximate data structures, limiting their view of network traffic. We argue that, due to the skewed nature of network traffic, such data structures are, in practice, heavily underutilized, i.e., sparse, thus wasting a significant amount of memory. This paper proposes a Sparse Approximate Data Structure (SPADA) representation that leverages sparsity to reduce the memory footprint of per-flow monitoring systems in the data plane while preserving their original accuracy. SPADA representation can be integrated into a generic per-flow monitoring system and is suitable for several measurement use cases. We prototype SPADA in P4 for a commercial FPGA target and test our approach with a custom simulator that we make publicly available, on four real network traces over three different monitoring tasks. Our results show that SPADA achieves 2× to 11× memory footprint reduction with respect to the state-of-the-art while maintaining the same accuracy, or even improving it.},
  howpublished = {/docs/2023SPADA.pdf},
  topic = {coda},
  series = {CoNEXT 23}
}


@inproceedings{AAAI-23,
  author = {Azorin, Raphael and Gallo, Massimo and Finamore, Alessandro and Rossi, Dario and Michiardi, Pietro},
  title = {"It's a Match!" - A Benchmark of Task Affinity Scores for Joint Learning},
  month={January},
  year = {2023},
  abstract = {While the promises of Multi-Task Learning (MTL) are attractive, characterizing the conditions of its success is still an open problem in Deep Learning. Some tasks may benefit from being learned together while others may be detrimental to one another. From a task perspective, grouping cooperative tasks while separating competing tasks is paramount to reap the benefits of MTL, i.e., reducing training and inference costs. Therefore, estimating task affinity for joint learning is a key endeavor. Recent work suggests that the training conditions themselves have a significant impact on the outcomes of MTL. Yet, the literature is lacking of a benchmark to assess the effectiveness of tasks affinity estimation techniques and their relation with actual MTL performance. In this paper, we take a first step in recovering this gap by (i) defining a set of affinity scores by both revisiting contributions from previous literature as well presenting new ones and (ii) benchmarking them on the Taskonomy dataset. Our empirical campaign reveals how, even in a small-scale scenario, task affinity scoring does not correlate well with actual MTL performance. Yet, some metrics can be more indicative than others.},   
  howpublished = {/docs/2023AAAIWorkshop.pdf},
  topic = {representation_learning},
  series = {AAAI-WS 23}
}
